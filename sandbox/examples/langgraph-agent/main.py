"""
LangGraph Agent with complete agent-sandbox integration.

This agent uses LangGraph's workflow-based architecture and integrates all sandbox tools
including file operations, code execution, shell commands, and browser automation.
"""

import os
import json
from typing import List, Dict, Any, TypedDict, Annotated, Union, Optional
from dataclasses import dataclass, field
from enum import Enum
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from agent_sandbox import Sandbox

load_dotenv()


class AgentState(TypedDict):
    """State for the LangGraph agent."""
    messages: List[BaseMessage]
    current_step: str
    tool_calls: List[Dict[str, Any]]
    tool_results: List[Dict[str, Any]]
    final_answer: str
    iterations: int


class AgentStep(Enum):
    """Steps in the agent workflow."""
    UNDERSTAND = "understand"
    PLAN = "plan"
    EXECUTE = "execute"
    REVIEW = "review"
    ANSWER = "answer"


# Import all tools from our comprehensive tools module
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from tools import (
    execute_python_code,
    execute_javascript_code,
    read_file,
    write_file,
    replace_in_file,
    search_in_file,
    find_files,
    list_directory,
    upload_file,
    download_file,
    execute_shell_command,
    create_shell_session,
    list_shell_sessions,
    cleanup_all_sessions,
    get_browser_info,
    take_screenshot,
    browser_navigate,
    browser_click,
    browser_type,
    browser_scroll,
    set_browser_resolution,
    convert_to_markdown,
)


# Define all available tools for the agent
all_tools = [
    # Code Execution
    execute_python_code,
    execute_javascript_code,
    
    # File Operations
    read_file,
    write_file,
    replace_in_file,
    search_in_file,
    find_files,
    list_directory,
    upload_file,
    download_file,
    
    # Shell Operations
    execute_shell_command,
    create_shell_session,
    list_shell_sessions,
    cleanup_all_sessions,
    
    # Browser Operations
    get_browser_info,
    take_screenshot,
    browser_navigate,
    browser_click,
    browser_type,
    browser_scroll,
    set_browser_resolution,
    
    # Utility
    convert_to_markdown,
]


tool_node = ToolNode(all_tools)


class SandboxClient:
    """Sandbox client wrapper."""
    
    def __init__(self):
        self.sandbox_url = os.getenv("SANDBOX_BASE_URL", "http://localhost:8080")
        self._sandbox = None
    
    @property
    def sandbox(self) -> Sandbox:
        if self._sandbox is None:
            self._sandbox = Sandbox(base_url=self.sandbox_url)
        return self._sandbox


sandbox_client = SandboxClient()


def create_llm():
    """Create LLM instance using Volcengine API."""
    api_key = os.getenv("COZE_WORKLOAD_IDENTITY_API_KEY")
    base_url = os.getenv("COZE_INTEGRATION_MODEL_BASE_URL")
    model = os.getenv("CODE_LLM_MODEL", "deepseek-v3-2-251201")
    
    if not api_key or not base_url:
        raise ValueError("Missing required environment variables: COZE_WORKLOAD_IDENTITY_API_KEY or COZE_INTEGRATION_MODEL_BASE_URL")
    
    return ChatOpenAI(
        model=model,
        base_url=base_url,
        api_key=api_key,
        temperature=0,
    )


def should_continue(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰§è¡Œæˆ–ç»“æŸ"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    if last_message and isinstance(last_message, AIMessage):
        if last_message.content.strip().endswith("FINAL_ANSWER:"):
            return "end"
        elif any(isinstance(msg, ToolMessage) for msg in messages):
            return "continue"
    
    return "continue"


def understand_node(state: AgentState) -> AgentState:
    """ç†è§£ç”¨æˆ·è¾“å…¥çš„èŠ‚ç‚¹"""
    llm = create_llm()
    messages = state["messages"]
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ éœ€è¦ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ã€‚

è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼š
1. é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. éœ€è¦æ‰§è¡Œä»€ä¹ˆæ“ä½œï¼Ÿ
3. éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·ï¼Ÿ

è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚"""
    
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        *messages
    ])
    
    return {
        "messages": state["messages"] + [response],
        "current_step": AgentStep.PLAN.value,
        "tool_calls": state.get("tool_calls", []),
        "tool_results": state.get("tool_results", []),
        "final_answer": "",
        "iterations": state.get("iterations", 0) + 1
    }


def plan_node(state: AgentState) -> AgentState:
    """è§„åˆ’æ‰§è¡Œæ­¥éª¤çš„èŠ‚ç‚¹"""
    llm = create_llm()
    messages = state["messages"]
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚åŸºäºå½“å‰çš„é—®é¢˜å’Œå·²æœ‰çš„ä¿¡æ¯ï¼Œè§„åˆ’ä¸‹ä¸€æ­¥éœ€è¦åšä»€ä¹ˆã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
PLANNED_ACTION: <ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆ>
USE_TOOL: <æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…· (yes/no)>
TOOL_NAME: <å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼Œå·¥å…·åç§°>
TOOL_ARGS: <å·¥å…·å‚æ•°ï¼ŒJSONæ ¼å¼>

å¦‚æœé—®é¢˜å·²ç»è§£å†³ï¼Œè¯·å›ç­”ï¼š
FINAL_ANSWER: <æœ€ç»ˆç­”æ¡ˆ>"""
    
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        *messages
    ])
    
    return {
        "messages": state["messages"] + [response],
        "current_step": AgentStep.EXECUTE.value,
        "tool_calls": state.get("tool_calls", []),
        "tool_results": state.get("tool_results", []),
        "final_answer": "",
        "iterations": state.get("iterations", 0)
    }


def execute_node(state: AgentState) -> AgentState:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨çš„èŠ‚ç‚¹"""
    messages = state["messages"]
    last_message = messages[-1] if messages else None
    
    if last_message and isinstance(last_message, AIMessage):
        if "FINAL_ANSWER:" in last_message.content:
            answer = last_message.content.replace("FINAL_ANSWER:", "").strip()
            return {
                "messages": state["messages"],
                "current_step": AgentStep.ANSWER.value,
                "tool_calls": state.get("tool_calls", []),
                "tool_results": state.get("tool_results", []),
                "final_answer": answer,
                "iterations": state.get("iterations", 0)
            }
    
    return state


def review_node(state: AgentState) -> AgentState:
    """å®¡æŸ¥ç»“æœçš„èŠ‚ç‚¹"""
    llm = create_llm()
    messages = state["messages"]
    tool_results = state.get("tool_results", [])
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·å®¡æŸ¥å·¥å…·æ‰§è¡Œçš„ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦å®Œæˆäº†ä»»åŠ¡ã€‚

å¦‚æœä»»åŠ¡å®Œæˆï¼Œè¯·å›ç­”ï¼š
FINAL_ANSWER: <æœ€ç»ˆç­”æ¡ˆ>

å¦‚æœä»»åŠ¡æœªå®Œæˆï¼Œè¯·ç»§ç»­è§„åˆ’ä¸‹ä¸€æ­¥æ“ä½œã€‚"""
    
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        *messages,
        SystemMessage(content=f"\nå·¥å…·æ‰§è¡Œç»“æœ: {tool_results}")
    ])
    
    return {
        "messages": state["messages"] + [response],
        "current_step": AgentStep.PLAN.value,
        "tool_calls": state.get("tool_calls", []),
        "tool_results": state.get("tool_results", []),
        "final_answer": "",
        "iterations": state.get("iterations", 0)
    }


def create_agent_graph() -> StateGraph:
    """åˆ›å»º LangGraph çŠ¶æ€å›¾"""
    workflow = StateGraph(AgentState)
    
    workflow.add_node("understand", understand_node)
    workflow.add_node("plan", plan_node)
    workflow.add_node("execute", execute_node)
    workflow.add_node("review", review_node)
    workflow.add_node("tools", tool_node)
    
    workflow.set_entry_point("understand")
    
    workflow.add_edge("understand", "plan")
    workflow.add_edge("plan", "execute")
    
    workflow.add_conditional_edges(
        "execute",
        lambda state: should_continue(state),
        {
            "continue": "tools",
            "end": END
        }
    )
    
    workflow.add_edge("tools", "review")
    workflow.add_edge("review", "plan")
    
    return workflow


def run_graph(query: str) -> Dict[str, Any]:
    """è¿è¡Œå›¾æ¨ç†"""
    app = create_agent_graph().compile()
    
    initial_state: AgentState = {
        "messages": [HumanMessage(content=query)],
        "current_step": AgentStep.UNDERSTAND.value,
        "tool_calls": [],
        "tool_results": [],
        "final_answer": "",
        "iterations": 0
    }
    
    config = {"configurable": {"thread_id": "1"}}
    
    final_state = None
    for state in app.stream(initial_state, config=config):
        final_state = state
        print(f"\nğŸ“ çŠ¶æ€æ›´æ–°: {list(state.keys())}")
    
    return final_state


def print_tools_info():
    """Print all available tools."""
    print("ğŸ“¦ å¯ç”¨å·¥å…·åˆ—è¡¨:")
    print("-" * 60)
    
    tool_categories = {
        "ğŸ ä»£ç æ‰§è¡Œ": ["execute_python_code", "execute_javascript_code"],
        "ğŸ“ æ–‡ä»¶æ“ä½œ": ["read_file", "write_file", "replace_in_file", "search_in_file", "find_files", "list_directory", "upload_file", "download_file"],
        "ğŸ’» Shellå‘½ä»¤": ["execute_shell_command", "create_shell_session", "list_shell_sessions", "cleanup_all_sessions"],
        "ğŸŒ æµè§ˆå™¨": ["get_browser_info", "take_screenshot", "browser_navigate", "browser_click", "browser_type", "browser_scroll", "set_browser_resolution"],
        "ğŸ”§ å·¥å…·": ["convert_to_markdown"],
    }
    
    for category, tool_names in tool_categories.items():
        print(f"\n{category}:")
        for tool_name in tool_names:
            print(f"  â€¢ {tool_name}")
    
    print("\n" + "=" * 60)


def main():
    """ä¸»å…¥å£"""
    print("ğŸš€ å¯åŠ¨ LangGraph Agent (å®Œæ•´å·¥å…·é›†æˆç‰ˆ)...")
    print("=" * 60)
    
    print_tools_info()
    
    print("\nğŸ’¬ Agent å·²å°±ç»ªï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜:")
    print("   (ç¤ºä¾‹: è®¡ç®— 1+1, åˆ—å‡º /tmp ç›®å½•, æ‰§è¡Œ shell å‘½ä»¤, ç­‰)")
    print("   è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("-" * 60)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            print("\nğŸ¤– æ‰§è¡Œä¸­...")
            result = run_graph(user_input)
            
            print("\nâœ… æœ€ç»ˆç»“æœ:")
            print("-" * 60)
            if result and "final_answer" in result:
                print(result["final_answer"])
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
